{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":["!conda install -y -c rdkit rdkit\n","!conda install -y -c conda-forge editdistance\n","!pip install deepsmiles\n","!pip install selfies"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":["from typing import List, Optional\n","import logging\n","from itertools import islice\n","\n","import yaml\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","import torchvision.io\n","from torchvision import transforms\n","import pytorch_lightning as pl\n","\n","import albumentations as A\n","import cv2\n","\n","from rdkit import Chem\n","from rdkit.Chem import inchi\n","import deepsmiles\n","import selfies\n","\n","try:\n","    from editdistance import eval as lsdistance\n","except ImportError:\n","    from Levenshtein import distance as lsdistance\n","\n","\n","# ============ CONFIG ============\n","\n","DATASET = \"/kaggle/input/bms-molecular-translation\"\n","TARGETS = \"/kaggle/input/bms-custom/train_labels.csv\"\n","TEST_PATH = \"/kaggle/input/bms-molecular-translation/sample_submission.csv\"\n","\n","CONFIG = \"\"\"\n","epochs: 1\n","target: selfies\n","imsize: 224\n","batch_size: 64\n","encoded_image_size: 14\n","input_grayscale: False\n","encoder_model: resnet18\n","encoder_fine_tune: False\n","encoder_pretrained: False\n","attention_dim: 256\n","embedding_dim: 256\n","decoder_dim: 512\n","encoder_dim: 512\n","decoder_dropout: 0.5\n","learning_rate: 0.004\n","max_grad_norm: 5\n","max_pred_len: 120\n","gpus: 1\n","\"\"\"\n","\n","\n","# ============ UTILS ============\n","\n","\n","def inchi2smiles(inchi_str: str):\n","    mol = inchi.MolFromInchi(inchi_str)\n","    return Chem.MolToSmiles(mol)\n","\n","\n","def smiles2inchi(smiles_str: str):\n","    mol = Chem.MolFromSmiles(smiles_str)\n","    if mol is not None:\n","        inchi_str = inchi.MolToInchi(mol)\n","    else:\n","        inchi_str = \"InChI=1S/\"\n","    return inchi_str\n","\n","\n","def smiles2deepsmiles(smiles: List[str], rings=True, branches=True):\n","    converter = deepsmiles.Converter(rings=rings, branches=branches)\n","    return [converter.encode(smile) for smile in smiles]\n","\n","\n","def deepsmiles2smiles(dsm: List[str], rings=True, branches=True):\n","    converter = deepsmiles.Converter(rings=rings, branches=branches)\n","    return [converter.decode(ds) for ds in dsm]\n","\n","\n","def smiles2selfies(smiles: List[str]):\n","    return [selfies.encoder(smile) for smile in smiles]\n","\n","\n","def selfies2smiles(selfies_list: List[str]):\n","    return [selfies.decoder(sf) for sf in selfies_list]\n","\n","\n","def selfies2inchi(selfies_str: str) -> str:\n","    smiles = selfies.decoder(selfies_str)\n","    inchi_str = smiles2inchi(smiles)\n","    return inchi_str\n","\n","\n","# ============ EVALUATION ============\n","\n","\n","def eval_ld_batch(y_true: List[str], y_pred: List[str]):\n","    lds = [lsdistance(y, y_) for y, y_ in zip(y_true, y_pred)]\n","    mean_ld = np.mean(lds)\n","    return mean_ld\n","\n","\n","class Evaluator:\n","\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","\n","    def eval_batch(self, idxs, y_pred):\n","        original_targets = self.dataset.get_original_targets(idxs)\n","        ld = eval_ld_batch(original_targets, y_pred)\n","        return ld\n","\n","\n","# ============ DATA ============\n","\n","\n","PRETRAINED_MEAN = [0.485, 0.456, 0.406]\n","PRETRAINED_STD = [0.229, 0.224, 0.225]\n","PRETRAINED_TEST_MEAN = [0.485, 0.456, 0.406]\n","PRETRAINED_TEST_STD = [0.229, 0.224, 0.225]\n","\n","\n","def get_img_file_path(\n","        image_id: str,\n","        split: str,\n","        dataset_path: str = \"../data/raw\"\n","):\n","    return \"{}/{}/{}/{}/{}/{}.png\".format(\n","        dataset_path, split, image_id[0], image_id[1], image_id[2], image_id\n","    )\n","\n","\n","def load_targets(targets_path: str, target: str) -> pd.DataFrame:\n","    usecols = list({\"image_id\", \"InChI\", target})\n","    df_targets = pd.read_csv(\n","        targets_path,\n","        usecols=usecols\n","    )\n","    df_targets.rename(columns={target: \"target\"}, inplace=True)\n","    return df_targets\n","\n","\n","class Tokenizer:\n","\n","    def __init__(self):\n","        self.stoi = dict()\n","        self.itos = dict()\n","        self.aux_tokens = ['<SOS>', '<EOS>', '<UNK>', '<PAD>']\n","\n","    def __len__(self):\n","        return len(self.stoi)\n","\n","    def fit(self, labels: List[str]):\n","        raise NotImplementedError()\n","\n","    def tokenize(self, label: str) -> List[int]:\n","        raise NotImplementedError()\n","\n","    def reverse_tokenize(self, idxs: List[int]) -> str:\n","        raise NotImplementedError()\n","\n","\n","class SelfiesTokenizer(Tokenizer):\n","\n","    def fit(self, labels: List[str]):\n","        logging.info(\"Constructing SELFIES vocabulary\")\n","        vocabulary = selfies.get_alphabet_from_selfies(tqdm(labels))\n","        vocabulary = sorted(list(vocabulary))\n","        vocabulary.extend(self.aux_tokens)\n","        self.stoi = {s: i for i, s in enumerate(vocabulary)}\n","        self.itos = dict(enumerate(vocabulary))\n","\n","    def tokenize(self, label: str) -> List[int]:\n","        unk_idx = self.stoi['<UNK>']\n","        tokens = selfies.split_selfies(label)\n","        idxs = [self.stoi.get(token, unk_idx) for token in tokens]\n","        return idxs\n","\n","    def reverse_tokenize(self, idxs: List[int], filter_aux: bool = True) -> str:\n","        tokens = map(self.itos.get, idxs)\n","        if filter_aux:\n","            tokens = filter(lambda t: t not in self.aux_tokens, tokens)\n","        return ''.join(tokens)\n","\n","\n","class MolecularCaptioningDataset(Dataset):\n","\n","    def __init__(\n","            self,\n","            dataset_path: str,\n","            df_targets: pd.DataFrame,\n","            tokenizer: Tokenizer,\n","            transforms\n","    ):\n","        super().__init__()\n","        self.dataset_path = dataset_path\n","        self.df_targets = df_targets\n","        self.tokenizer = tokenizer\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return self.df_targets.shape[0]\n","\n","    def __getitem__(self, idx):\n","        row = self.df_targets.iloc[idx]\n","        image_id, target = row[\"image_id\"], row[\"target\"]\n","        img_path = get_img_file_path(\n","            image_id,\n","            \"train\",\n","            dataset_path=self.dataset_path\n","        )\n","        # img = torchvision.io.read_image(img_path)\n","        # img = img / 255.0\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        img = self.transforms(img)\n","        tgt_sequence = self.tokenizer.tokenize(target)\n","        tgt_len = torch.LongTensor([len(tgt_sequence)])\n","        tgt_sequence = torch.LongTensor(tgt_sequence)\n","        return img, tgt_sequence, tgt_len\n","\n","    def collate_fn(self, batch):\n","        imgs, labels, label_lengths = [], [], []\n","        for data_point in batch:\n","            imgs.append(data_point[0])\n","            labels.append(data_point[1])\n","            label_lengths.append(data_point[2])\n","        labels = pad_sequence(labels, batch_first=True, padding_value=self.tokenizer.stoi[\"<PAD>\"])\n","        return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)\n","\n","\n","class MolecularCaptioningValDataset(MolecularCaptioningDataset):\n","\n","    def __getitem__(self, idx):\n","        img, tgt_sequence, tgt_len = super(MolecularCaptioningValDataset, self).__getitem__(idx)\n","        return img, tgt_sequence, tgt_len, torch.LongTensor([idx])\n","\n","    def collate_fn(self, batch):\n","        imgs, labels, label_lengths = super(MolecularCaptioningValDataset, self).collate_fn(batch)\n","        idxs = []\n","        for data_point in batch:\n","            idxs.append(data_point[-1])\n","        return imgs, labels, label_lengths, torch.stack(idxs).reshape(-1, 1)\n","\n","    def get_original_targets(self, idxs):\n","        return self.df_targets.iloc[idxs][\"InChI\"]\n","\n","\n","class MolecularCaptioningTestDataset(Dataset):\n","\n","    def __init__(\n","            self,\n","            dataset_path: str,\n","            df_image_ids: pd.DataFrame,\n","            transforms\n","    ):\n","        self.dataset_path = dataset_path\n","        self.df_image_ids = df_image_ids\n","        self.transforms = transforms\n","        self.fix_transform = A.Compose([A.Transpose(p=1), A.VerticalFlip(p=1)])\n","\n","    def __len_(self):\n","        return self.df_image_ids.shape[0]\n","\n","    def __getitem__(self, idx):\n","        image_id = self.df_image_ids.iloc[idx]['image_id']\n","        img_path = get_img_file_path(\n","            image_id,\n","            \"test\",\n","            dataset_path=self.dataset_path\n","        )\n","        # img = torchvision.io.read_image(img_path)\n","        # img = img / 255.0\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        _, h, w = img.size()\n","        if h > w:\n","            img = self.fix_transform(image=img)['image']\n","        img = self.transforms(img)\n","        return img\n","\n","\n","class MolecularCaptioningDataModule(pl.LightningDataModule):\n","\n","    def __init__(\n","            self,\n","            dataset_path: str,\n","            target: str,\n","            targets_path: str,\n","            test_ids_path: str,\n","            imsize: int,\n","            batch_size: int,\n","            num_workers: int = 4,\n","            train_size: float = 0.8\n","    ):\n","        super().__init__()\n","        self.dataset_path = dataset_path\n","        self.targets_path = targets_path\n","        self.imsize = imsize\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.train_size = train_size\n","        self.df_targets = load_targets(targets_path, target)\n","        self.df_test_ids = pd.read_csv(test_ids_path)\n","        self.tokenizer = SelfiesTokenizer()\n","        self.train, self.val, self.test = None, None, None\n","        self.df_train, self.df_val = None, None\n","\n","    def prepare_data(self):\n","        pass\n","\n","    def setup(self, stage: Optional[str] = None):\n","        if stage == 'fit' or stage is None:\n","            df_train, df_val = train_test_split(\n","                self.df_targets,\n","                train_size=self.train_size,\n","                shuffle=True,\n","                random_state=42\n","            )\n","            self.df_train, self.df_val = df_train, df_val\n","            self.tokenizer.fit(df_train['target'])\n","            self.train = MolecularCaptioningDataset(\n","                self.dataset_path,\n","                df_targets=df_train,\n","                tokenizer=self.tokenizer,\n","                transforms=self._init_tforms(\"train\")\n","            )\n","            self.val = MolecularCaptioningValDataset(\n","                self.dataset_path,\n","                df_targets=df_val,\n","                tokenizer=self.tokenizer,\n","                transforms=self._init_tforms(\"test\")\n","            )\n","        if stage == 'test' or stage is None:\n","            self.test = MolecularCaptioningTestDataset(\n","                self.dataset_path,\n","                df_image_ids=self.df_test_ids,\n","                transforms=self._init_tforms(\"test\")\n","            )\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train,\n","            batch_size=self.batch_size,\n","            num_workers=self.num_workers,\n","            shuffle=True,\n","            drop_last=True,\n","            pin_memory=True,\n","            collate_fn=self.train.collate_fn\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val,\n","            batch_size=4,\n","            num_workers=self.num_workers,\n","            shuffle=False,\n","            drop_last=False,\n","            pin_memory=True,\n","            collate_fn=self.val.collate_fn\n","        )\n","\n","    def test_dataloader(self):\n","        return DataLoader(\n","            self.test,\n","            batch_size=1024,\n","            num_workers=self.num_workers,\n","            shuffle=False,\n","            drop_last=False,\n","        )\n","\n","    def _init_tforms(self, stage: str):\n","        tforms = None\n","        if stage == 'train':\n","            tforms = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(2 * (self.imsize,)),\n","                transforms.Normalize(\n","                    mean=PRETRAINED_MEAN,\n","                    std=PRETRAINED_STD\n","                )\n","            ])\n","        elif stage == 'test':\n","            tforms = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(2 * (self.imsize,)),\n","                transforms.Normalize(\n","                    mean=PRETRAINED_TEST_MEAN,\n","                    std=PRETRAINED_TEST_STD\n","                )\n","            ])\n","        return tforms\n","\n","    def _aug_tfroms(self):\n","        return transforms.Compose([\n","            transforms.RandomErasing(p=0.2),\n","            transforms.RandomHorizontalFlip(p=0.2),\n","            transforms.RandomApply([transforms.RandomRotation(90)])\n","        ])\n","\n","\n","# ============= MODULES =============\n","\n","class Encoder(nn.Module):\n","\n","    def __init__(\n","            self,\n","            encoded_image_size: int,\n","            is_grayscale: bool,\n","            fine_tune: bool = True,\n","            model: str = 'resnet18',\n","            pretrained: bool = False\n","    ):\n","        \"\"\"\n","        :param encoded_image_size: output tensor WxH dimensions\n","        :param fine_tune: enable fine-tuning of residual blocks 2-4\n","        :param model: model name corresponding to the model from torchvision library\n","        :param pretrained: load pretrained weights\n","        \"\"\"\n","        super(Encoder, self).__init__()\n","        self.encoded_image_size = encoded_image_size\n","        self.is_grayscale = is_grayscale\n","        self.fine_tune = fine_tune\n","        self.model = model\n","        self.pretrained = pretrained\n","        self.encoder_net = self._make_encoder_net()\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n","        if self.pretrained:\n","            self.encoder_net = self._fine_tune_params(self.encoder_net, fine_tune)\n","\n","    def _make_encoder_net(self):\n","        model_builder = getattr(torchvision.models, self.model)\n","        resnet = model_builder(pretrained=self.pretrained)\n","        modules = list(resnet.children())[:-2]\n","        resnet = nn.Sequential(*modules)\n","        if self.is_grayscale:\n","            resnet.conv1 = nn.Conv2d(\n","                1, 64,\n","                kernel_size=(7, 7),\n","                stride=(2, 2),\n","                padding=(3, 3),\n","                bias=False\n","            )\n","        return resnet\n","\n","    def _fine_tune_params(self, resnet: nn.Module, fine_tune: bool):\n","        for param in resnet.parameters():\n","            param.requires_grad = False\n","        for child in islice(resnet.children(), 5, None):\n","            for param in child.parameters():\n","                param.requires_grad = fine_tune\n","        return resnet\n","\n","    def forward(self, x):\n","        y = self.encoder_net(x)\n","        y = y.permute(0, 2, 3, 1)\n","        return y\n","\n","\n","class Attention(nn.Module):\n","\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        \"\"\"\n","        :param encoder_dim: feature size of encoded images\n","        :param decoder_dim: size of decoder's RNN\n","        :param attention_dim: size of the attention network\n","        \"\"\"\n","        super(Attention, self).__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n","        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        \"\"\"\n","        Forward propagation.\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n","        :return: attention weighted encoding, weights\n","        \"\"\"\n","        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n","        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n","        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n","        alpha = self.softmax(att)  # (batch_size, num_pixels)\n","        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n","\n","        return attention_weighted_encoding, alpha\n","\n","\n","class DecoderWithAttention(nn.Module):\n","    \"\"\"\n","    Decoder.\n","    \"\"\"\n","\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, device, dropout=0.5):\n","        \"\"\"\n","        :param attention_dim: size of attention network\n","        :param embed_dim: embedding size\n","        :param decoder_dim: size of decoder's RNN\n","        :param vocab_size: size of vocabulary\n","        :param encoder_dim: feature size of encoded images\n","        :param dropout: dropout\n","        \"\"\"\n","        super(DecoderWithAttention, self).__init__()\n","\n","        self.encoder_dim = encoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.decoder_dim = decoder_dim\n","        self.vocab_size = vocab_size\n","        self.dropout = dropout\n","        self.device = device\n","\n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n","        self.dropout = nn.Dropout(p=self.dropout)\n","        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n","        self.sigmoid = nn.Sigmoid()\n","        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n","        self.init_weights()  # initialize some layers with the uniform distribution\n","\n","    def init_weights(self):\n","        \"\"\"\n","        Initializes some parameters with values from the uniform distribution, for easier convergence.\n","        \"\"\"\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.fc.bias.data.fill_(0)\n","        self.fc.weight.data.uniform_(-0.1, 0.1)\n","\n","    def load_pretrained_embeddings(self, embeddings):\n","        \"\"\"\n","        Loads embedding layer with pre-trained embeddings.\n","        :param embeddings: pre-trained embeddings\n","        \"\"\"\n","        self.embedding.weight = nn.Parameter(embeddings)\n","\n","    def fine_tune_embeddings(self, fine_tune=True):\n","        \"\"\"\n","        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n","        :param fine_tune: Allow?\n","        \"\"\"\n","        for p in self.embedding.parameters():\n","            p.requires_grad = fine_tune\n","\n","    def init_hidden_state(self, encoder_out):\n","        \"\"\"\n","        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        :return: hidden state, cell state\n","        \"\"\"\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out)\n","        return h, c\n","\n","    def forward(self, encoder_out, encoded_captions, caption_lengths):\n","        \"\"\"\n","        Forward propagation.\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n","        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n","        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n","        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n","        \"\"\"\n","\n","        batch_size = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1)\n","        vocab_size = self.vocab_size\n","\n","        # Flatten image\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","        num_pixels = encoder_out.size(1)\n","\n","        # Sort input data by decreasing lengths; why? apparent below\n","        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n","        encoder_out = encoder_out[sort_ind]\n","        encoded_captions = encoded_captions[sort_ind]\n","\n","        # Embedding\n","        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n","\n","        # Initialize LSTM state\n","        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n","\n","        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n","        # So, decoding lengths are actual lengths - 1\n","        decode_lengths = (caption_lengths - 1).tolist()\n","\n","        # Create tensors to hold word predicion scores and alphas\n","        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).type_as(h)\n","        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).type_as(h)\n","\n","        # At each time-step, decode by\n","        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n","        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n","        for t in range(max(decode_lengths)):\n","            batch_size_t = sum([l > t for l in decode_lengths])\n","            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n","                                                                h[:batch_size_t])\n","            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","            h, c = self.decode_step(\n","                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n","                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n","            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n","            predictions[:batch_size_t, t, :] = preds\n","            alphas[:batch_size_t, t, :] = alpha\n","\n","        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n","\n","    def decode(self, encoder_out, decode_lengths, tokenizer):\n","        batch_size = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1)\n","        vocab_size = self.vocab_size\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","        num_pixels = encoder_out.size(1)\n","        # embed start tocken for LSTM input\n","        start_tockens = torch.ones(batch_size, dtype=torch.long, device=self.embedding.weight.device) * tokenizer.stoi[\"<SOS>\"]\n","        embeddings = self.embedding(start_tockens)\n","        # initialize hidden state and cell state of LSTM cell\n","        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n","        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).type_as(h)\n","        # predict sequence\n","        for t in range(decode_lengths):\n","            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n","            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","            h, c = self.decode_step(\n","                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n","                (h, c))  # (batch_size_t, decoder_dim)\n","            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n","            predictions[:, t, :] = preds\n","            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<EOS>\"]:\n","                break\n","            embeddings = self.embedding(torch.argmax(preds, -1))\n","        return predictions\n","\n","\n","# ============= MODELS =============\n","\n","\n","class MolecularCaptioningModel(pl.LightningModule):\n","\n","    def __init__(\n","            self,\n","            learning_rate,\n","            encoded_image_size,\n","            input_grayscale,\n","            encoder_model,\n","            encoder_fine_tune,\n","            encoder_pretrained,\n","            attention_dim,\n","            embedding_dim,\n","            decoder_dim,\n","            encoder_dim,\n","            decoder_dropout,\n","            tokenizer,\n","            target,\n","            translate_fn,\n","            evaluator,\n","            max_pred_len=120\n","    ):\n","        super(MolecularCaptioningModel, self).__init__()\n","        self.save_hyperparameters(\n","            \"learning_rate\",\n","            \"encoded_image_size\",\n","            \"input_grayscale\",\n","            \"encoder_model\",\n","            \"encoder_fine_tune\",\n","            \"encoder_pretrained\",\n","            \"attention_dim\",\n","            \"embedding_dim\",\n","            \"decoder_dim\",\n","            \"encoder_dim\",\n","            \"decoder_dropout\",\n","            \"target\",\n","            \"max_pred_len\"\n","        )\n","        self.learning_rate = learning_rate\n","        self.encoder = Encoder(\n","            encoded_image_size=encoded_image_size,\n","            is_grayscale=input_grayscale,\n","            model=encoder_model,\n","            fine_tune=encoder_fine_tune,\n","            pretrained=encoder_pretrained\n","        )\n","        self.decoder = DecoderWithAttention(\n","            attention_dim=attention_dim,\n","            embed_dim=embedding_dim,\n","            decoder_dim=decoder_dim,\n","            vocab_size=len(tokenizer),\n","            encoder_dim=encoder_dim,\n","            dropout=decoder_dropout,\n","            device=self.device\n","        )\n","        self.max_pred_len = max_pred_len\n","        self.tokenizer = tokenizer\n","        self.evaluator = evaluator\n","        self.target = target\n","        self.translate_fn = translate_fn\n","        self.criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.stoi[\"<PAD>\"])\n","\n","    def forward(self, images):\n","        pass\n","\n","    def _compute_loss(self, images, labels, label_lengths):\n","        features = self.encoder(images)\n","        predictions_, caps_sorted, decode_lengths, alphas, sort_ind = self.decoder(features, labels, label_lengths)\n","        targets = caps_sorted[:, 1:]\n","        predictions = pack_padded_sequence(predictions_, decode_lengths, batch_first=True).data\n","        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n","        loss = self.criterion(predictions, targets)\n","        return loss, features\n","\n","    def training_step(self, batch, batch_idx):\n","        images, labels, label_lengths = batch\n","        loss, _ = self._compute_loss(images, labels, label_lengths)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        images, labels, label_lengths, idxs = batch\n","        loss, encoded = self._compute_loss(images, labels, label_lengths)\n","        predictions = self.decoder.decode(encoded, self.max_pred_len, self.tokenizer)\n","        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n","        predicted_sequences = [self.tokenizer.reverse_tokenize(seq) for seq in predicted_sequence]\n","        translated_sequences = [self.translate_fn(seq) for seq in predicted_sequences]\n","        cv_ld = self.evaluator.eval_batch(idxs.detach().cpu().numpy().ravel(), translated_sequences)\n","        self.log('val_loss', loss)\n","        self.log('cv_ld', cv_ld)\n","\n","    def configure_optimizers(self):\n","        optimizer = Adam(\n","            self.parameters(),\n","            lr=self.learning_rate,\n","            amsgrad=False\n","        )\n","        scheduler = ReduceLROnPlateau(\n","            optimizer,\n","            mode='min',\n","            factor=0.2,\n","            patience=4,\n","            eps=1e-6,\n","            verbose=True\n","        )\n","        return {\n","            'optimizer': optimizer,\n","            'lr_scheduler': scheduler,\n","            'monitor': 'val_loss'\n","        }\n","\n","\n","# ============= TRAINING =============\n","\n","pl.seed_everything(42)\n","conf = yaml.safe_load(CONFIG)\n","datamodule = MolecularCaptioningDataModule(\n","    dataset_path=DATASET,\n","    target=conf['target'],\n","    targets_path=TARGETS,\n","    test_ids_path=TEST_PATH,\n","    imsize=conf['imsize'],\n","    batch_size=conf['batch_size']\n",")\n","datamodule.setup('fit')\n","evaluator = Evaluator(datamodule.val)\n","model = MolecularCaptioningModel(\n","    learning_rate=conf['learning_rate'],\n","    encoded_image_size=conf['encoded_image_size'],\n","    input_grayscale=conf['input_grayscale'],\n","    encoder_model=conf['encoder_model'],\n","    encoder_fine_tune=conf['encoder_fine_tune'],\n","    encoder_pretrained=conf['encoder_pretrained'],\n","    attention_dim=conf['attention_dim'],\n","    embedding_dim=conf['embedding_dim'],\n","    decoder_dim=conf['decoder_dim'],\n","    encoder_dim=conf['encoder_dim'],\n","    decoder_dropout=conf['decoder_dropout'],\n","    tokenizer=datamodule.tokenizer,\n","    target=conf['target'],\n","    translate_fn=selfies2inchi,\n","    max_pred_len=conf['max_pred_len'],\n","    evaluator=evaluator\n",")\n","trainer = pl.Trainer(\n","    default_root_dir='/kaggle/working',\n","    deterministic=True,\n","    gpus=conf['gpus'],\n","    max_epochs=conf['epochs'],\n","    gradient_clip_val=conf['max_grad_norm'],\n","    precision=16\n",")\n","trainer.fit(model, datamodule)\n"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}